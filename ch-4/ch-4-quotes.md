###SYSTEMS OF SCALE
What's accross the ocean

Rob Emmanuele
My recent talk about geo big data frameworks at LocationTech: https://www.youtube.com/watch?v=WIkEZKUj4hU
FOSS4G, it’s regional conferences (including FOSS4G NA 2016, which I would shamelessly plug as I’m the program chair https://2016.foss4g-na.org/cfp), and any conference by either OSGeo and LocationTech would be rich in talk about big geo data. The LocationTech working group under the Eclipse Foundation specifically hosts 4 big geo data processing frameworks, and there’s a lot of synergy happening around geo big data there.

PLANET OS

For instance, PlanetLabs wants a daily global image of the world. That is a lot of imagery data. How do we process that data to, say, extract tree lines and calculate deforestation? There is just one challenge and opportunity.


Global Forest Watch

CURRENT TEXT FROM DRAFT

TOOLS (Incorporate tools into table)

* TensorFlow (""which can crudely be accurately described as a raster-to-semantic-vector converter"")
* AWS 
* Hadoop / Hive / Presto / 
* Spark OS - not particularly useful by default for geo, but can be extended with GeoTrellis and other utilities that will like evolve.
* R + Python for analysis
* Tableau + Stamen
* PostgreSQL/PostGIS
* Mapnik
* CartoDB
* OpenLayers
* ArcGIS / QGIS 
* TatukGIS
* Satellite Data: PostGIS, GDAL, OpenCV and Numpy
* Kubernetes is an open source container cluster manager originally designed by Google and donated to the Cloud Native Computing Foundation. It aims to provide a "platform for automating deployment, scaling, and operations of application containers across clusters of hosts"
* GeoTrellis for raster,  provides geospatial capabilities to Scala and the Apache Spark framework
	* Raster: Spark, Hadoop, Accumulo, Amazon Web Services features like S3 and EC2
* Magellen is an option for vector but overlooking topological problems and onley doing points stuff
* GeoServer/Networ, OL, LeafLet, OpenGIS, PostGIS, Oracle + Geo cartridge, JSTS
* City/Govt: Google Fusion Tables + Google Maps + Windows OS

TECHNIQUES

* Inverse Distance Weighting - interpolate for areas with few samples (IDW)
* PostGIS function extension
* Geohashing
* OGC = Open Geospatial Consortium
* TINs are vector-based models depicting three-dimensional elevation surface terrains. Using three dimensional coordinates (x, y and z), TINs connect vertices and form non-overlapping planar triangles.

IOT/MICROSTATS

At a certain level of data production, devices both passively scan swaths of the earth, like drones and satellites, and also generate data which we will increasingly need to process, geocode, and coalate in real time. Image analsysis and feature extraction will become increasingly important, as the kind of integrity and detail we glean from images is so much more thorough than collections of geocoordinates for the plotting


**Boris Shimanovsky, Factual**
TOOLS

"We’ve open-sourced libraries for working with geohashes and polygons.  The venerable JTS library is used a lot."

IOT/SAT/MICROSTAT

"It will also create more uncharted territory where security, privacy laws, and tools to understand the data will lag behind."

EVOLUTION FROM SMALL BEGINNINGS



"The open source ecosystem is great, and is, in fact, likely superior to the commercial offerings which seem behind the times.  Tools like Postgis, Solr, and Elasticsearch should not be overlooked as people overestimate the big-ness of their data.  Geospatial data, at true big data scale, doesn’t yet have an obvious silver bullet technology that makes everything great and easy.  It will, in any event, require very capable (and likely expensive) engineers to harness.  It’s hard for me to imagine a general purpose big data tool that can abstract away the geo-specific knowledge required to analyze this data efficiently."

**Tyler Bell, Factual (referred by DJ Patil), now Mapbox**

"'Big Data' was a handy moniker for a while, but the sector's vocabulary, business process, and data pipelines have matured to the extent that the term lack of specificity can sometimes hinder problem solving and critical thinking, so I try to avoid it where I can.   With that pedantry aside, I would note that these large-scale, high-volume approaches to data analytics are brilliantly suited to any use case or business problem when massive data sets must be regularly analyzed for currency, accuracy, and coverage, and where global datasets must have local precision.  This translates comfortably to highly granular maps of the word (or patterns of human movement globally) that have been generated by disparate, intermittent, and often-untrusted data points.  This matters because we -- technologists, but humans more broadly -- are moving away from the world of monolithic data collection, to one where small signals, each one perhaps insignificant by itself, are combined to create the spatial intelligence of the future.  Every car is becoming a Google Streetview car and every consume a sensor.
"

"The problems really boil down coverage, quality, and creation: how we analyze global coverage and quality to prioritize engineering and BD energies; how we develop software that better joins human- and machine-based efforts to correct and protect the map; and how we work with sensor and probe data to enhance the and update the map in real-time.
"

IOT/SAT/MICROSTAT
""**All of these devices are capturing data and annotating it geographically, or capturing geo data and annotating it contextually;  In one sense these are all 'maps that move', but really their availability is both enriching the quality and multiplying the breadth sensor coverage.**""


**Riley Newcomb, Airbnb**

ANALYTICS
"the phenomena surrounding and complicating what is interesting about geodata naturally draws us to want more processing power, the ability to aggregate and compare more disparate datasets, the ability to inform one mapped trend with another...."it’s at the heart of the information revolution underway.""

TOOLS

""Our standard set of tools and technologies are: AWS/hadoop/hive/presto/spark (data infrastructure), R & Python for analysis, Tableau for run-of-the-mill visualizations (especially since their collaboration with Stamen design for geo plotting), and then specialized tools for custom visualizations (Rbnb/Airpy, D3, Processing, Tilemill)
Also worth discussing: core_data, the knowledge repo, the ERF (all homegrown tools)
""

OPEN DATA

"SF’s OpenData project is a great example. Making data public can lead to all kinds of projects that benefit the community without requiring public resources. It lowers the barrier to entry for NGOs and crowdsources innovative solutions to public issues. The more of this we can do, the better."


**Robin Kraft, Space.Ninja and GlobalForestWatch.org**

OPEN DATA
"ESRI is the Microsoft of the geospatial world, for better or for worse, and it’s very expensive stuff. You can do a lot of great stuff with OSS, and gov’t agencies should be more open to that.
"

**Rainer Sternfeld, Founder + CEO, Planet OS**

"Big data and geospatial analytics require surprisingly little explanation these days, as to why this is important. The key is to be on point when it comes to use cases and benefits.

The way we explain it depends on whether we are talking to specific industry CxOs or IT professionals.
"


PROBLEMS

"We have been focused on two problem spaces: data supply infrastructure for government agencies that publish massive-scale data to the industries, and data dashboards for the energy industry. Specifically, how do you make it easy for users to discover, access and use weather, climate, environmental, fisheries data in a machine-readable way? And on the industry side, we’ve been involved in issues such as: how do you operate modern offshore wind farms, manage the lifecycle of jet engines, environmental monitoring of refineries and offshore drilling platforms?
"

TOOLS
"We build on Akka clustering which gives us a robust environment for deploying and scaling our system. Most of our geo-tools are home-grown on top of standard file formats like NetCDF."

""Apache Spark lets us do the analysis we need to to understand and index some of the massive datasets we process.
HBase gives us a platform for storing and retrieval the results of distributed computations
ElasticSearch gives us a nice platform for free-text and polygonal retrieval
Mesos and various projects built on Mesos give us a stable compute infrastructure.

While members of our team contribute to various open source projects (such as D3 and Clojure), our contributions in the geo/big data have been small so far. Hopefully we will increase our activity here over the next year or so. PlanetOS has begun releasing front-end code such as the cirrusjs library for visualization (https://planetos.com/blog/cirrus-js/) and we will continue to release visualization, data processing, and analytics code as we see the opportunity.""

"**To date, the open source tools for geospatial processing in the big data arena have been very limited in scope and not really “industrial grade” (for example, Climate Corporation’s Mandoline system for managing tiled satellite data and Factual’s geo tools).**

I see this beginning to change with broader platform initiatives such as HortonWorks’ Magellan geospatial framework for Apache Spark and the Pyro spatial-temporal database. I’m optimistic that the next few years will begin to see more fully featured geospatial processing platforms integrated with Spark in particular as well.

Another thing I would like to see, though I don’t know if anyone is working on it is the big data equivalent to PostGIS. Pyro may provide some of this (it at least tried to address some common issues), but I don’t know enough about it yet."

**Stefan Avesand, Ericsson Smartphone Labs**

TOOLS

"We use a large flora of tools at Ericsson, both commercial and open source. These include e.g. PostgreSQL/PostGIS, Hadoop/Hive, Mapnik, CartoDB, OpenLayers, ArcGIS, TatukGIS and Tableau.

**Rob Emmanuele, Azavea/Geotrellis**
"GeoTrellis was a Scala library for doing large scale raster operations." 
	"There is a need for the ability to do processing on large raster data in a distributed manner, or medium size raster data in a very fast manner. We’ve been leveraging Spark, Hadoop, Accumulo, Amazon Web Services features like S3 and EC2."
	"Spark has been the single most useful tooling that I’ve found when working with these types of problems. Spark however does not have geospatial capabilities by default, and that is what GeoTrellis is providing to an extent, and aiming to fully provide down the road."

"By taking advantage of Spark and Amazon Web Services, we were able to scale up our raster processing to much larger datasets."\

BIG DATA

"The **Landsat satellite program**, which has been taking images of the earth since 1970, has an archive that is most certainly big data. The size of the imagery hosted to the public for free on Amazon Web Services, which only hosts 2015 Landsat data as well as select scenes from 2014 and 2013, amounts to over 250 Terabytes. If you imagine the largest hard drive you’ve heard of, you can try to think of how many of those hard drives it would take to hold all of Landsat’s entire imagery set. But “Big Data” is not just about how large the data set is. It’s about how we deal with it. How we store it, and how we process it. There’s a whole ecosystem of tools for dealing with data at this scale, and are known as “Big Data” frameworks or “Big Data” platforms.
"